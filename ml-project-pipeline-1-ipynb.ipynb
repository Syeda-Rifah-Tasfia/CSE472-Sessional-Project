{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":192692,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":164314,"modelId":186648},{"sourceId":192974,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":164570,"modelId":186898}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install kaggle timm --quiet\n\n!mkdir -p ~/.kaggle\n!echo '{\"username\":\"hafijulhoque987 \",\"key\":\"ba267fc402273b17f82059844f85fe32\"}' > ~/.kaggle/kaggle.json\n!chmod 600 ~/.kaggle/kaggle.json\n\n# Download the dataset\n!kaggle datasets download -d andrewmvd/pediatric-pneumonia-chest-xray\n\n# Extract the dataset\nimport zipfile\n\nwith zipfile.ZipFile('pediatric-pneumonia-chest-xray.zip', 'r') as zip_ref:\n    zip_ref.extractall('./pediatric_pneumonia')\n\nprint(\"Dataset downloaded and extracted successfully!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:54:22.253029Z","iopub.execute_input":"2024-12-09T02:54:22.253875Z","iopub.status.idle":"2024-12-09T02:54:44.641631Z","shell.execute_reply.started":"2024-12-09T02:54:22.253829Z","shell.execute_reply":"2024-12-09T02:54:44.640528Z"}},"outputs":[{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/andrewmvd/pediatric-pneumonia-chest-xray\nLicense(s): Attribution 4.0 International (CC BY 4.0)\npediatric-pneumonia-chest-xray.zip: Skipping, found more recently modified local copy (use --force to force download)\nDataset downloaded and extracted successfully!\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import torch\n# Set up device for training (GPU if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:55:01.167286Z","iopub.execute_input":"2024-12-09T02:55:01.168230Z","iopub.status.idle":"2024-12-09T02:55:01.174086Z","shell.execute_reply.started":"2024-12-09T02:55:01.168176Z","shell.execute_reply":"2024-12-09T02:55:01.173205Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import copy\nimport math\nfrom typing import List, Optional\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\n\nclass PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n\n    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        if scale is not None and normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n        if scale is None:\n            scale = 2 * math.pi\n        self.scale = scale\n\n        # Multi-head attention module\n        # self.attention = nn.MultiheadAttention(embed_dim=2 * num_pos_feats, num_heads=num_heads)\n\n    def forward(self, x, mask=None):\n        if mask is None:\n            mask = torch.zeros((x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool)\n        not_mask = ~mask\n        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n        if self.normalize:\n            eps = 1e-6\n            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n\n        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n\n        pos_x = x_embed[:, :, :, None] / dim_t\n        pos_y = y_embed[:, :, :, None] / dim_t\n        pos_x = torch.stack(\n            (pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4\n        ).flatten(3)\n        pos_y = torch.stack(\n            (pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4\n        ).flatten(3)\n        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n        return pos\n\n        \n        # # Flatten for attention: [batch_size, 2*num_pos_feats, height, width] -> [height*width, batch_size, 2*num_pos_feats]\n        # pos_flat = pos.flatten(2).permute(2, 0, 1)\n\n        # # Apply multi-head attention\n        # pos_weighted, _ = self.attention(pos_flat, pos_flat, pos_flat)\n\n        # # Reshape back to [batch_size, 2*num_pos_feats, height, width]\n        # pos_weighted = pos_weighted.permute(1, 2, 0).view(pos.size(0), pos.size(1), pos.size(2), pos.size(3))\n\n        # return pos_weighted\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        d_model=512,\n        nhead=8,\n        num_encoder_layers=6,\n        num_decoder_layers=6,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"relu\",\n        normalize_before=False,\n        return_intermediate_dec=False,\n    ):\n        super().__init__()\n\n        encoder_layer = TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, activation, normalize_before\n        )\n        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n\n        decoder_layer = TransformerDecoderLayer(\n            d_model, nhead, dim_feedforward, dropout, activation, normalize_before\n        )\n        decoder_norm = nn.LayerNorm(d_model)\n        self.decoder = TransformerDecoder(\n            decoder_layer,\n            num_decoder_layers,\n            decoder_norm,\n            return_intermediate=return_intermediate_dec,\n        )\n\n        self._reset_parameters()\n\n        self.d_model = d_model\n        self.nhead = nhead\n\n    def _reset_parameters(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, src, mask, query_embed, query_pos, value_pos):\n        # flatten NxCxHxW to HWxNxC\n        bs, c, l = query_embed.shape\n        query_embed = query_embed.permute(2, 0, 1)\n        query_pos = query_pos.unsqueeze(0).expand(bs,-1,-1).permute(2, 0, 1)\n        value=src.flatten(2).permute(2, 0, 1)\n        value_pos=value_pos.flatten(2).permute(2, 0, 1)\n        if mask is not None:\n            mask = mask.flatten(1)\n\n        #tgt = torch.zeros_like(query_embed)\n        memory = self.encoder(value, src_key_padding_mask=mask, pos=value_pos)\n        hs = self.decoder(\n            tgt=query_embed, memory=memory, memory_key_padding_mask=mask, pos=value_pos, query_pos=query_pos\n        )\n        return hs.transpose(1, 2).transpose(0, 2)\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, encoder_layer, num_layers, norm=None):\n        super().__init__()\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n\n    def forward(\n        self,\n        src,\n        mask: Optional[Tensor] = None,\n        src_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n    ):\n        output = src\n\n        for layer in self.layers:\n            output = layer(\n                output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, pos=pos\n            )\n\n        if self.norm is not None:\n            output = self.norm(output)\n\n        return output\n\n\nclass TransformerDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n        self.return_intermediate = return_intermediate\n\n    def forward(\n        self,\n        tgt,\n        memory,\n        tgt_mask: Optional[Tensor] = None,\n        memory_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        memory_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n        query_pos: Optional[Tensor] = None,\n    ):\n        output = tgt\n\n        intermediate = []\n\n        for layer in self.layers:\n            output = layer(\n                tgt=output,\n                memory=memory,\n                tgt_mask=tgt_mask,\n                memory_mask=memory_mask,\n                tgt_key_padding_mask=tgt_key_padding_mask,\n                memory_key_padding_mask=memory_key_padding_mask,\n                pos=pos,\n                query_pos=query_pos,\n            )\n            if self.return_intermediate:\n                intermediate.append(self.norm(output))\n\n        if self.norm is not None:\n            output = self.norm(output)\n            if self.return_intermediate:\n                intermediate.pop()\n                intermediate.append(output)\n\n        if self.return_intermediate:\n            return torch.stack(intermediate)\n\n        return output\n\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"relu\",\n        normalize_before=False,\n    ):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n\n    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n        return tensor if pos is None else tensor + pos\n\n    def forward_post(\n        self,\n        src,\n        src_mask: Optional[Tensor] = None,\n        src_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n    ):\n        q = k = self.with_pos_embed(src, pos)\n        src2 = self.self_attn(\n            q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n        )[0]\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src\n\n    def forward_pre(\n        self,\n        src,\n        src_mask: Optional[Tensor] = None,\n        src_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n    ):\n        src2 = self.norm1(src)\n        q = k = self.with_pos_embed(src2, pos)\n        src2 = self.self_attn(\n            q, k, value=src2, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n        )[0]\n        src = src + self.dropout1(src2)\n        src2 = self.norm2(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n        src = src + self.dropout2(src2)\n        return src\n\n    def forward(\n        self,\n        src,\n        src_mask: Optional[Tensor] = None,\n        src_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n    ):\n        if self.normalize_before:\n            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n\n\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"relu\",\n        normalize_before=False,\n    ):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n\n    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n        return tensor if pos is None else tensor + pos\n\n    def forward_post(\n        self,\n        tgt,\n        memory,\n        tgt_mask: Optional[Tensor] = None,\n        memory_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        memory_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n        query_pos: Optional[Tensor] = None,\n    ):\n        q = k = self.with_pos_embed(tgt, query_pos)\n        tgt2 = self.self_attn(\n            q, k, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask\n        )[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(\n            query=self.with_pos_embed(tgt, query_pos),\n            key=self.with_pos_embed(memory, pos),\n            value=memory,\n            attn_mask=memory_mask,\n            key_padding_mask=memory_key_padding_mask,\n        )[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt\n\n    def forward_pre(\n        self,\n        tgt,\n        memory,\n        tgt_mask: Optional[Tensor] = None,\n        memory_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        memory_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n        query_pos: Optional[Tensor] = None,\n    ):\n        tgt2 = self.norm1(tgt)\n        q = k = self.with_pos_embed(tgt2, query_pos)\n        tgt2 = self.self_attn(\n            q, k, value=tgt2, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask\n        )[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt2 = self.norm2(tgt)\n        tgt2 = self.multihead_attn(\n            query=self.with_pos_embed(tgt2, query_pos),\n            key=self.with_pos_embed(memory, pos),\n            value=memory,\n            attn_mask=memory_mask,\n            key_padding_mask=memory_key_padding_mask,\n        )[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt2 = self.norm3(tgt)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n        tgt = tgt + self.dropout3(tgt2)\n        return tgt\n\n    def forward(\n        self,\n        tgt,\n        memory,\n        tgt_mask: Optional[Tensor] = None,\n        memory_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        memory_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n        query_pos: Optional[Tensor] = None,\n    ):\n        if self.normalize_before:\n            return self.forward_pre(\n                tgt,\n                memory,\n                tgt_mask,\n                memory_mask,\n                tgt_key_padding_mask,\n                memory_key_padding_mask,\n                pos,\n                query_pos,\n            )\n        return self.forward_post(\n            tgt,\n            memory,\n            tgt_mask,\n            memory_mask,\n            tgt_key_padding_mask,\n            memory_key_padding_mask,\n            pos,\n            query_pos,\n        )\n\n\ndef _get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n\n\ndef _get_activation_fn(activation):\n    \"\"\"Return an activation function given a string\"\"\"\n    if activation == \"relu\":\n        return F.relu\n    if activation == \"gelu\":\n        return F.gelu\n    if activation == \"glu\":\n        return F.glu\n    raise RuntimeError(f\"activation should be relu/gelu, not {activation}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:55:04.568835Z","iopub.execute_input":"2024-12-09T02:55:04.569138Z","iopub.status.idle":"2024-12-09T02:55:04.609326Z","shell.execute_reply.started":"2024-12-09T02:55:04.569113Z","shell.execute_reply":"2024-12-09T02:55:04.608471Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels, out_channels, feature_dims):\n        super(UNet, self).__init__()\n\n        self.encoder1 = DoubleConv(in_channels, feature_dims)\n        self.encoder2 = DoubleConv(feature_dims, feature_dims * 2)\n        self.encoder3 = DoubleConv(feature_dims * 2, feature_dims * 4)\n        self.encoder4 = DoubleConv(feature_dims * 4, feature_dims * 8)\n\n        self.embed = nn.Embedding(feature_dims * 16,1)\n\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.pe_layer = PositionEmbeddingSine(feature_dims * 16 // 2, normalize=True)\n        self.transformer = Transformer(\n            d_model=feature_dims * 16,\n            dropout=0.1,\n            nhead=4,\n            dim_feedforward=feature_dims * 4,\n            num_encoder_layers=0,\n            num_decoder_layers=1,\n            normalize_before=False,\n            return_intermediate_dec=False,\n        )\n\n        self.bottleneck_conv = DoubleConv(feature_dims * 8, feature_dims * 16)\n\n        self.upconv4 = nn.ConvTranspose2d(feature_dims * 16, feature_dims * 8, kernel_size=2, stride=2)\n        self.decoder4 = DoubleConv(feature_dims * 16, feature_dims * 8)\n        self.upconv3 = nn.ConvTranspose2d(feature_dims * 8, feature_dims * 4, kernel_size=2, stride=2)\n        self.decoder3 = DoubleConv(feature_dims * 8, feature_dims * 4)\n        self.upconv2 = nn.ConvTranspose2d(feature_dims * 4, feature_dims * 2, kernel_size=2, stride=2)\n        self.decoder2 = DoubleConv(feature_dims * 4, feature_dims * 2)\n        self.upconv1 = nn.ConvTranspose2d(feature_dims * 2, feature_dims, kernel_size=2, stride=2)\n        self.decoder1 = DoubleConv(feature_dims * 2, feature_dims)\n\n        self.final_conv = nn.Conv2d(feature_dims, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        # Encoder\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(self.pool(enc1))\n        enc3 = self.encoder3(self.pool(enc2))\n        enc4 = self.encoder4(self.pool(enc3))\n\n        # Bottleneck\n        bottleneck = self.pool(enc4)\n        bottleneck = self.bottleneck_conv(bottleneck)\n\n        b, c, h, w = bottleneck.shape\n        bottleneck_avg = bottleneck.mean(dim=(2, 3), keepdim=True).squeeze(-1)\n        bottleneck_pe = self.pe_layer(bottleneck)\n\n        transformer_output = self.transformer(bottleneck, None, bottleneck_avg, self.embed.weight, bottleneck_pe) # (bsz,1024,1)\n\n        transformer_output_expanded = transformer_output.unsqueeze(-1) # (bsz,1024,1,1)\n        bottleneck = bottleneck * transformer_output_expanded\n\n        # Decoder\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n        return torch.sigmoid(self.final_conv(dec1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:55:07.937820Z","iopub.execute_input":"2024-12-09T02:55:07.938396Z","iopub.status.idle":"2024-12-09T02:55:07.951998Z","shell.execute_reply.started":"2024-12-09T02:55:07.938363Z","shell.execute_reply":"2024-12-09T02:55:07.951160Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.models import resnet50, resnet101\nfrom torchvision.models._utils import IntermediateLayerGetter\n\nclass FrozenBatchNorm2d(torch.nn.Module):\n    def __init__(self, n):\n        super(FrozenBatchNorm2d, self).__init__()\n        self.register_buffer(\"weight\", torch.ones(n))\n        self.register_buffer(\"bias\", torch.zeros(n))\n        self.register_buffer(\"running_mean\", torch.zeros(n))\n        self.register_buffer(\"running_var\", torch.ones(n))\n\n    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs):\n        num_batches_tracked_key = prefix + 'num_batches_tracked'\n        if num_batches_tracked_key in state_dict:\n            del state_dict[num_batches_tracked_key]\n\n        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict,\n            missing_keys, unexpected_keys, error_msgs)\n\n    def forward(self, x):\n        w = self.weight.reshape(1, -1, 1, 1)\n        b = self.bias.reshape(1, -1, 1, 1)\n        rv = self.running_var.reshape(1, -1, 1, 1)\n        rm = self.running_mean.reshape(1, -1, 1, 1)\n        eps = 1e-5\n        scale = w * (rv + eps).rsqrt()\n        bias = b - rm * scale\n        return x * scale + bias\n\n\nclass BackboneBase(nn.Module):\n    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n        super().__init__()\n        for name, parameter in backbone.named_parameters():\n            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n                parameter.requires_grad_(False)\n        if return_interm_layers:\n            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n        else:\n            return_layers = {'layer4': \"0\"}\n        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n        self.num_channels = num_channels\n\n    def forward(self, x):\n        x = self.body(x)\n        return x\n\nresnets_dict = {\n    'resnet50': resnet50,\n    'resnet101': resnet101,\n}\n\nclass Backbone(BackboneBase):\n    def __init__(self, name: str, train_backbone: bool, return_interm_layers: bool, dilation: list):\n        backbone = resnets_dict[name](\n            pretrained=True, replace_stride_with_dilation=dilation, norm_layer=FrozenBatchNorm2d\n        )\n        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n        super().__init__(backbone, train_backbone, num_channels, return_interm_layers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:55:25.655688Z","iopub.execute_input":"2024-12-09T02:55:25.656385Z","iopub.status.idle":"2024-12-09T02:55:25.667335Z","shell.execute_reply.started":"2024-12-09T02:55:25.656353Z","shell.execute_reply":"2024-12-09T02:55:25.666397Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass MSR(nn.Module):\n    def __init__(self, layers, num_classes=2, reduce_dim=256):\n        super(MSR, self).__init__()\n        self.backbone = Backbone(\n            'resnet{}'.format(layers),\n            train_backbone=False,\n            return_interm_layers=True,\n            dilation=[False, True, True]\n        )\n        self.embed_cat=nn.Embedding(reduce_dim,1)\n        self.embed_3=nn.Embedding(reduce_dim,1)\n        self.pe_layer_cat=PositionEmbeddingSine(reduce_dim//2, normalize=True)\n        self.pe_layer_3=PositionEmbeddingSine(reduce_dim//2, normalize=True)\n\n        self.transformer_cat = Transformer(\n            d_model=reduce_dim,\n            dropout=0.1,\n            nhead=4,\n            dim_feedforward=reduce_dim//4,\n            num_encoder_layers=0,\n            num_decoder_layers=1,\n            normalize_before=False,\n            return_intermediate_dec=False,\n        )\n\n        self.transformer_3 = Transformer(\n            d_model=reduce_dim,\n            dropout=0.1,\n            nhead=4,\n            dim_feedforward=reduce_dim//4,\n            num_encoder_layers=0,\n            num_decoder_layers=1,\n            normalize_before=False,\n            return_intermediate_dec=False,\n        )\n        self.conv_red_1 = nn.Sequential(\n            nn.Conv2d(512, reduce_dim//2, kernel_size=1, padding=0, bias=False)\n        )\n        self.conv_red_2 = nn.Sequential(\n            nn.Conv2d(1024, reduce_dim//2, kernel_size=1, padding=0, bias=False)\n        )\n        self.conv_red_3 = nn.Sequential(\n            nn.Conv2d(2048, reduce_dim, kernel_size=1, padding=0, bias=False)\n        )\n\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(reduce_dim*2, reduce_dim)  # Assuming last feature map size\n        self.fc_2 = nn.Linear(reduce_dim, num_classes)  # Assuming last feature map size\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        # Backbone feature extraction\n        back_x = self.backbone(x)\n        red_back_1 = self.conv_red_1(back_x['1'])\n        red_back_2 = self.conv_red_2(back_x['2'])\n\n        red_back_cat = torch.cat((red_back_1, red_back_2), dim=1)\n        avg_back_cat = self.avgpool(red_back_cat)\n\n        red_back_3 = self.conv_red_3(back_x['3'])\n        avg_back_3 = self.avgpool(red_back_3)\n\n        masking=None\n        query_pos = self.embed_cat.weight\n\n        key_embed = red_back_cat\n        query_embed = avg_back_cat.squeeze(-1)\n        key_pos = self.pe_layer_cat(red_back_cat)\n\n        fg_embed_cat=self.transformer_cat(key_embed,masking,query_embed,query_pos,key_pos)\n\n        query_pos = self.embed_3.weight\n        key_embed = red_back_3\n        query_embed = avg_back_3.squeeze(-1)\n        key_pos = self.pe_layer_3(red_back_3)\n        fg_embed_3=self.transformer_3(key_embed,masking,query_embed,query_pos,key_pos)\n\n\n        #out_back_cat = (torch.einsum(\"bchw,bcl->blhw\",red_back_cat,fg_embed_cat)).permute(0, 2, 3, 1)\n        #out_back_3 = (torch.einsum(\"bchw,bcl->blhw\",red_back_3,fg_embed_3)).permute(0, 2, 3, 1)\n\n\n\n        # Concatenate along the last dimension\n        out = torch.cat((fg_embed_cat, fg_embed_3), dim=1)\n\n        out_1 = torch.flatten(out, 1)  # Flatten the feature maps\n\n        out_1 = self.fc(out_1)  # Fully connected layer\n        out_1 = self.fc_2(out_1)  # Fully connected layer\n        return out_1\n\n    def get_backbone_params(self):\n        return self.backbone.parameters()\n\n    def get_fc_params(self):\n        return self.fc.parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:55:28.371592Z","iopub.execute_input":"2024-12-09T02:55:28.371901Z","iopub.status.idle":"2024-12-09T02:55:28.383894Z","shell.execute_reply.started":"2024-12-09T02:55:28.371877Z","shell.execute_reply":"2024-12-09T02:55:28.382934Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\n\ndef get_data_loader(data_dir, batch_size):\n    # Define transformations\n    train_transform = transforms.Compose([\n        transforms.Resize((512, 512)),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor()\n    ])\n\n    test_transform = transforms.Compose([\n        transforms.Resize((512, 512)),\n        transforms.ToTensor()\n    ])\n\n    # Load datasets\n    train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"train\"), transform=train_transform)\n    test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"test\"), transform=test_transform)\n\n\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n\n\n    return train_loader, test_loader\n\ndef get_test_loader(data_dir, batch_size):\n    transform = transforms.Compose([\n        transforms.Resize((512, 512)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # Load test dataset\n    test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"test\"), transform=transform)\n\n\n    # Create test loader\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n\n\n    return test_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:55:32.054027Z","iopub.execute_input":"2024-12-09T02:55:32.054850Z","iopub.status.idle":"2024-12-09T02:55:32.062675Z","shell.execute_reply.started":"2024-12-09T02:55:32.054816Z","shell.execute_reply":"2024-12-09T02:55:32.061707Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:55:36.212808Z","iopub.execute_input":"2024-12-09T02:55:36.213601Z","iopub.status.idle":"2024-12-09T02:55:44.317282Z","shell.execute_reply.started":"2024-12-09T02:55:36.213567Z","shell.execute_reply":"2024-12-09T02:55:44.316370Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation-models-pytorch in /opt/conda/lib/python3.10/site-packages (0.3.4)\nRequirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.7.1)\nRequirement already satisfied: huggingface-hub>=0.24.6 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.26.2)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.3.0)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.7.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (1.16.0)\nRequirement already satisfied: timm==0.9.7 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.9.7)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.19.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.4.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import pickle\nimport os\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport shutil  # For directory management\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n\n# Load the trained model from a .pkl file\nmodel_path = \"/kaggle/input/seg_bestmodel_base/pytorch/default/1/best_model_base.pth\"\nmodel = UNet(in_channels=3, out_channels=3, feature_dims=16)  # Match your training parameters\nmodel.load_state_dict(torch.load(model_path))\nmodel.to(device)  # Move the model to the appropriate device (CPU or GPU)\n\nprint(\"Model loaded successfully from .pkl\")\n\nfor name, param in model.named_parameters():\n    print(f\"{name}: mean={param.mean().item()}, std={param.std().item()}\")\n\nfrom pathlib import Path\nPath('/kaggle/working/Kermany_masks/train').mkdir(parents=True, exist_ok=True)\nPath('/kaggle/working/Kermany_masks/test').mkdir(parents=True, exist_ok=True)        \n\n\n\ndef generate_masks(model, data_loader, device, output_dir, threshold=0.5):\n\n    # # Ensure the output directory is fresh each time\n    # # if os.path.exists(output_dir):\n    # #     shutil.rmtree(output_dir)\n    # if not os.path.exists(output_dir):\n    #     os.makedirs(output_dir)\n    os.makedirs(os.path.join(output_dir, \"NORMAL\"), exist_ok=True)\n    os.makedirs(os.path.join(output_dir, \"PNEUMONIA\"), exist_ok=True)\n\n    model.eval()\n    model.to(device)\n\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(data_loader):\n            # Convert inputs to grayscale if necessary\n          #  inputs = transforms.functional.rgb_to_grayscale(inputs, num_output_channels=1)\n\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            predictions = (outputs > threshold).float()\n\n\n            for j in range(inputs.size(0)):\n                label = targets[j].item()  # Get the integer label (0 or 1)\n\n                # Determine the save directory based on the label\n                if label == 0:\n                    class_dir = os.path.join(output_dir, \"NORMAL\")\n                elif label == 1:\n                    class_dir = os.path.join(output_dir, \"PNEUMONIA\")\n                else:\n                    raise ValueError(f\"Unexpected label value: {label}\")\n\n                # Get the original image filename\n                original_path, _ = data_loader.dataset.samples[batch_idx * data_loader.batch_size + j]\n                image_name = os.path.basename(original_path)  # Extract the original filename\n                #print(\"Image name\", image_name)\n                save_path = os.path.join(class_dir, image_name.replace(\".jpg\", \"_mask.png\"))\n\n                # Save the predicted mask with the original filename\n                pred_mask = predictions[j].cpu().numpy().squeeze()\n                pred_mask = (pred_mask > threshold).astype(np.uint8)\n                if pred_mask.ndim == 3:\n                    pred_mask = pred_mask[0]\n                 # Plot the mask for debugging\n                # plt.figure(figsize=(10, 5))\n                # plt.subplot(1, 2, 1)\n                # plt.title(\"Predicted Mask\")\n                # plt.imshow(pred_mask, cmap='gray')\n                # plt.axis(\"off\")\n\n                # plt.subplot(1, 2, 2)\n                # plt.title(\"Original Image\")\n                # plt.imshow(inputs[j].cpu().permute(1, 2, 0).numpy())\n                # plt.axis(\"off\")\n\n                # plt.show()\n\n\n                pred_mask = (pred_mask * 255).astype(np.uint8)  # Scale to 0-255\n                Image.fromarray(pred_mask).save(save_path)\n\n\n\n# Example Usage\n# data_dir = \"./kermany/kermany\"  # Ensure this is where your dataset is located\ndata_dir = \"/kaggle/working/pediatric_pneumonia/Pediatric Chest X-ray Pneumonia\"\nbatch_size = 8\n#one for train one for test\noutput_dir = \"/kaggle/working/Kermany_masks/test\"\noutput_dir1 = \"/kaggle/working/Kermany_masks/train\"\n\n# Assuming `segmentation_model` is your pre-trained model\ntrain_loader, test_loader = get_data_loader(data_dir, batch_size)\ngenerate_masks(model, test_loader, device, output_dir)\nprint(\"Test.\")\ngenerate_masks(model, train_loader, device, output_dir1)\nprint(\"Train\")\n\nprint(\"Output generated.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:55:47.194194Z","iopub.execute_input":"2024-12-09T02:55:47.194577Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully from .pkl\nencoder1.double_conv.0.weight: mean=0.001531185582280159, std=0.10937564820051193\nencoder1.double_conv.0.bias: mean=-0.031190697103738785, std=0.08863301575183868\nencoder1.double_conv.1.weight: mean=0.9967350959777832, std=0.024089418351650238\nencoder1.double_conv.1.bias: mean=0.007471294142305851, std=0.047271981835365295\nencoder1.double_conv.3.weight: mean=0.0038688203785568476, std=0.05116163566708565\nencoder1.double_conv.3.bias: mean=0.01079418696463108, std=0.03678091615438461\nencoder1.double_conv.4.weight: mean=0.9912546873092651, std=0.02626999281346798\nencoder1.double_conv.4.bias: mean=-0.007584636565297842, std=0.03577863797545433\nencoder2.double_conv.0.weight: mean=-0.0013007867382839322, std=0.05242982879281044\nencoder2.double_conv.0.bias: mean=-0.0038535005878657103, std=0.056800853461027145\nencoder2.double_conv.1.weight: mean=1.0004634857177734, std=0.03462415188550949\nencoder2.double_conv.1.bias: mean=-0.007000797428190708, std=0.027110524475574493\nencoder2.double_conv.3.weight: mean=-0.0011761481873691082, std=0.03922334313392639\nencoder2.double_conv.3.bias: mean=-0.0026746082585304976, std=0.036169253289699554\nencoder2.double_conv.4.weight: mean=0.9955500960350037, std=0.023592449724674225\nencoder2.double_conv.4.bias: mean=-0.016879916191101074, std=0.017012765631079674\nencoder3.double_conv.0.weight: mean=-0.002448021899908781, std=0.038958799093961716\nencoder3.double_conv.0.bias: mean=-0.003658988745883107, std=0.032790523022413254\nencoder3.double_conv.1.weight: mean=0.9974781274795532, std=0.0241794865578413\nencoder3.double_conv.1.bias: mean=-0.010558042675256729, std=0.02204608917236328\nencoder3.double_conv.3.weight: mean=-0.001610326929949224, std=0.030532481148838997\nencoder3.double_conv.3.bias: mean=0.0015577543526887894, std=0.02493130788207054\nencoder3.double_conv.4.weight: mean=0.9973809123039246, std=0.025003256276249886\nencoder3.double_conv.4.bias: mean=-0.02031949907541275, std=0.016499241814017296\nencoder4.double_conv.0.weight: mean=-0.0024624262005090714, std=0.03144383430480957\nencoder4.double_conv.0.bias: mean=-0.003091320628300309, std=0.024156128987669945\nencoder4.double_conv.1.weight: mean=0.9957518577575684, std=0.030958984047174454\nencoder4.double_conv.1.bias: mean=-0.017185484990477562, std=0.02233872376382351\nencoder4.double_conv.3.weight: mean=-0.0014680009335279465, std=0.026424139738082886\nencoder4.double_conv.3.bias: mean=0.0018878414994105697, std=0.016339344903826714\nencoder4.double_conv.4.weight: mean=0.9988899230957031, std=0.03534270077943802\nencoder4.double_conv.4.bias: mean=-0.027671106159687042, std=0.02321173995733261\nembed.weight: mean=0.10985948145389557, std=1.0298594236373901\ntransformer.decoder.layers.0.self_attn.in_proj_weight: mean=3.3427029848098755e-05, std=0.04474900662899017\ntransformer.decoder.layers.0.self_attn.in_proj_bias: mean=-2.4791632313281298e-05, std=0.005029486026614904\ntransformer.decoder.layers.0.self_attn.out_proj.weight: mean=0.00015436788089573383, std=0.06399255245923996\ntransformer.decoder.layers.0.self_attn.out_proj.bias: mean=0.0007421484333463013, std=0.012075372971594334\ntransformer.decoder.layers.0.multihead_attn.in_proj_weight: mean=-0.00029765802901238203, std=0.04774358868598938\ntransformer.decoder.layers.0.multihead_attn.in_proj_bias: mean=-0.0003125604707747698, std=0.013233009725809097\ntransformer.decoder.layers.0.multihead_attn.out_proj.weight: mean=2.987557672895491e-05, std=0.06419014185667038\ntransformer.decoder.layers.0.multihead_attn.out_proj.bias: mean=0.001471861731261015, std=0.015727253630757332\ntransformer.decoder.layers.0.linear1.weight: mean=0.0010746215702965856, std=0.07938256859779358\ntransformer.decoder.layers.0.linear1.bias: mean=-0.0038969849701970816, std=0.03957787901163101\ntransformer.decoder.layers.0.linear2.weight: mean=0.0010267638135701418, std=0.0794532373547554\ntransformer.decoder.layers.0.linear2.bias: mean=0.003874212270602584, std=0.07826355844736099\ntransformer.decoder.layers.0.norm1.weight: mean=0.9987624883651733, std=0.01860051229596138\ntransformer.decoder.layers.0.norm1.bias: mean=0.0009670860017649829, std=0.016492269933223724\ntransformer.decoder.layers.0.norm2.weight: mean=0.9876495003700256, std=0.025407694280147552\ntransformer.decoder.layers.0.norm2.bias: mean=0.001554529881104827, std=0.02178901992738247\ntransformer.decoder.layers.0.norm3.weight: mean=0.9817707538604736, std=0.029409758746623993\ntransformer.decoder.layers.0.norm3.bias: mean=0.0038824090734124184, std=0.026372116059064865\ntransformer.decoder.norm.weight: mean=0.9729185700416565, std=0.02713772840797901\ntransformer.decoder.norm.bias: mean=-0.005138637963682413, std=0.022875487804412842\nbottleneck_conv.double_conv.0.weight: mean=-0.0017063816776499152, std=0.027693010866642\nbottleneck_conv.double_conv.0.bias: mean=0.0006142997881397605, std=0.016984116286039352\nbottleneck_conv.double_conv.1.weight: mean=0.9911673665046692, std=0.03226625919342041\nbottleneck_conv.double_conv.1.bias: mean=-0.04185815528035164, std=0.02225668728351593\nbottleneck_conv.double_conv.3.weight: mean=-0.0013184953713789582, std=0.023481978103518486\nbottleneck_conv.double_conv.3.bias: mean=0.00018843484576791525, std=0.011814076453447342\nbottleneck_conv.double_conv.4.weight: mean=0.9782810211181641, std=0.022502189502120018\nbottleneck_conv.double_conv.4.bias: mean=-0.030889080837368965, std=0.018763558939099312\nupconv4.weight: mean=-0.00021992203255649656, std=0.030043764039874077\nupconv4.bias: mean=0.0014702563639730215, std=0.10451992601156235\ndecoder4.double_conv.0.weight: mean=-0.0006835553795099258, std=0.024495206773281097\ndecoder4.double_conv.0.bias: mean=0.0019350580405443907, std=0.011854026466608047\ndecoder4.double_conv.1.weight: mean=0.9887593984603882, std=0.03610311448574066\ndecoder4.double_conv.1.bias: mean=-0.028367534279823303, std=0.030538463965058327\ndecoder4.double_conv.3.weight: mean=-0.003593296743929386, std=0.028141260147094727\ndecoder4.double_conv.3.bias: mean=0.0019624666310846806, std=0.016529759392142296\ndecoder4.double_conv.4.weight: mean=0.9797574281692505, std=0.02841370552778244\ndecoder4.double_conv.4.bias: mean=-0.027158495038747787, std=0.024052411317825317\nupconv3.weight: mean=0.00021283300884533674, std=0.039941124618053436\nupconv3.bias: mean=0.014873872511088848, std=0.11390635371208191\ndecoder3.double_conv.0.weight: mean=-0.0006280457018874586, std=0.026483621448278427\ndecoder3.double_conv.0.bias: mean=0.0023864624090492725, std=0.016457952558994293\ndecoder3.double_conv.1.weight: mean=0.9949421882629395, std=0.02978360280394554\ndecoder3.double_conv.1.bias: mean=-0.03800230845808983, std=0.025997139513492584\ndecoder3.double_conv.3.weight: mean=-0.003000495722517371, std=0.03231003135442734\ndecoder3.double_conv.3.bias: mean=-1.6375561244785786e-06, std=0.025580964982509613\ndecoder3.double_conv.4.weight: mean=1.0009362697601318, std=0.021465452387928963\ndecoder3.double_conv.4.bias: mean=-0.019809354096651077, std=0.021478809416294098\nupconv2.weight: mean=0.0010924753732979298, std=0.05491821840405464\nupconv2.bias: mean=0.0051951720379292965, std=0.16562312841415405\ndecoder2.double_conv.0.weight: mean=-0.0021880664862692356, std=0.03080521710216999\ndecoder2.double_conv.0.bias: mean=0.0024460861459374428, std=0.023918598890304565\ndecoder2.double_conv.1.weight: mean=0.9977908134460449, std=0.019437603652477264\ndecoder2.double_conv.1.bias: mean=-0.013956231065094471, std=0.03237628564238548\ndecoder2.double_conv.3.weight: mean=-0.005792015697807074, std=0.038634732365608215\ndecoder2.double_conv.3.bias: mean=-0.011708788573741913, std=0.03611278161406517\ndecoder2.double_conv.4.weight: mean=1.0382859706878662, std=0.03248077258467674\ndecoder2.double_conv.4.bias: mean=-0.005414724349975586, std=0.035265859216451645\nupconv1.weight: mean=-0.005430804565548897, std=0.082672618329525\nupconv1.bias: mean=-0.054092489182949066, std=0.268573522567749\ndecoder1.double_conv.0.weight: mean=-0.0006898705614730716, std=0.04032473266124725\ndecoder1.double_conv.0.bias: mean=0.0038605807349085808, std=0.03754784166812897\ndecoder1.double_conv.1.weight: mean=0.9965758323669434, std=0.026378676295280457\ndecoder1.double_conv.1.bias: mean=0.0343213826417923, std=0.029991094022989273\ndecoder1.double_conv.3.weight: mean=0.0010535315377637744, std=0.05056968331336975\ndecoder1.double_conv.3.bias: mean=-0.0016215243376791477, std=0.044822607189416885\ndecoder1.double_conv.4.weight: mean=1.4309344291687012, std=0.09914831817150116\ndecoder1.double_conv.4.bias: mean=0.4539296627044678, std=0.08055085688829422\nfinal_conv.weight: mean=-0.20544838905334473, std=0.3495437800884247\nfinal_conv.bias: mean=-0.08254341781139374, std=0.19575047492980957\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/431176841.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"},{"name":"stdout","text":"Test.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Customized data pre-processing","metadata":{}},{"cell_type":"code","source":"# import os\n# from torchvision import datasets, transforms\n# from torch.utils.data import DataLoader, random_split\n# from PIL import Image\n# import torch\n\n# class CombinedDataset(torch.utils.data.Dataset):\n#     def __init__(self, data_dir, mask_dir, transform_img, transform_mask):\n#         self.image_dataset = datasets.ImageFolder(root=data_dir, transform=None)\n#         self.mask_dir = mask_dir\n#         self.transform_img = transform_img\n#         self.transform_mask = transform_mask\n\n#         self.samples = self.image_dataset.samples\n\n#     def __len__(self):\n#         return len(self.image_dataset)\n\n#     def __getitem__(self, index):\n#         img_path, label = self.samples[index]\n#         img = Image.open(img_path).convert('RGB')\n#         if self.transform_img:\n#             img = self.transform_img(img)\n\n#         # Match mask by exact filename\n#         class_name = os.path.basename(os.path.dirname(img_path))\n#         mask_path = os.path.join(self.mask_dir, class_name, os.path.basename(img_path))\n#         if not os.path.exists(mask_path):\n#             raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n\n#         mask = Image.open(mask_path).convert('L')  # Grayscale for masks\n#         if self.transform_mask:\n#             mask = self.transform_mask(mask)\n\n#         # Convert single-channel mask to 3-channel\n#         if mask.shape[0] == 1:\n#             mask = mask.repeat(3, 1, 1)\n\n#         # Convert single-channel image to 3-channel if necessary\n#         if img.shape[0] == 1:\n#             img = img.repeat(3, 1, 1)\n\n#         # Combine image with mask\n#         mask_rgb = torch.stack([mask.squeeze(0)] * 3, dim=0)\n#         img_with_mask = img * mask_rgb\n\n#         return img_with_mask, label\n\n# def get_combined_data_loaders(data_dir, mask_dir, batch_size):\n#     # Define transformations\n#     train_transform_img = transforms.Compose([\n#         transforms.Resize((512, 512)),\n#         transforms.RandomRotation(degrees=15),\n#         transforms.ColorJitter(brightness=0.2, contrast=0.2),\n#         transforms.ToTensor(),\n#     ])\n#     train_transform_mask = transforms.Compose([\n#         transforms.Resize((512, 512)),\n#         transforms.RandomRotation(degrees=15),\n#         transforms.ToTensor(),\n#     ])\n\n#     test_transform_img = transforms.Compose([\n#         transforms.Resize((512, 512)),\n#         transforms.ToTensor(),\n#     ])\n#     test_transform_mask = transforms.Compose([\n#         transforms.Resize((512, 512)),\n#         transforms.ToTensor(),\n#     ])\n\n#     # Define directories\n#     train_data_dir = os.path.join(data_dir, \"train\")\n#     test_data_dir = os.path.join(data_dir, \"test\")\n#     train_mask_dir = os.path.join(mask_dir, \"train\")\n#     test_mask_dir = os.path.join(mask_dir, \"test\")\n\n#     # Instantiate datasets\n#     train_dataset = CombinedDataset(train_data_dir, train_mask_dir, train_transform_img, train_transform_mask)\n#     test_dataset = CombinedDataset(test_data_dir, test_mask_dir, test_transform_img, test_transform_mask)\n\n#     # Split train dataset into train and validation sets\n#     train_size = int(0.8 * len(train_dataset))\n#     val_size = len(train_dataset) - train_size\n#     train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n\n#     # Create DataLoaders\n#     train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n#     validation_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n#     return train_loader, validation_loader, test_loader\n\n\n# # Example usage\n# data_dir = \"/kaggle/working/pediatric_pneumonia/Pediatric Chest X-ray Pneumonia\"  # Path to images\n# mask_dir = \"/kaggle/working/Kermany_masks\"   # Path to masks\n# batch_size = 8\n\n# train_loader, validation_loader, test_loader = get_combined_data_loaders(data_dir, mask_dir, batch_size)\n\n# print(\"Success\")\n\nfrom torch.utils.data import random_split\n\ndef get_combined_data_loaders(data_dir, mask_dir, batch_size):\n    image_transform = transforms.Compose([\n        transforms.Resize((512, 512)),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor()\n    ])\n\n    mask_transform = transforms.Compose([\n        transforms.Resize((512, 512)),\n        transforms.ToTensor()\n    ])\n    test_transform = transforms.Compose([\n        transforms.Resize((512, 512)),\n        transforms.ToTensor()\n    ])\n\n    class CombinedDataset(torch.utils.data.Dataset):\n        def __init__(self, data_dir, mask_dir, transform_img, transform_mask):\n            self.image_dataset = datasets.ImageFolder(root=data_dir, transform=None)\n            self.mask_dir = mask_dir\n            self.transform_img = transform_img\n            self.transform_mask = transform_mask\n\n            self.samples = self.image_dataset.samples\n\n        def __len__(self):\n            return len(self.image_dataset)\n\n        def __getitem__(self, index):\n            img_path, label = self.samples[index]\n            img = Image.open(img_path).convert('RGB')\n            if self.transform_img:\n                img = self.transform_img(img)\n\n            # Match mask by exact filename\n            class_name = os.path.basename(os.path.dirname(img_path))\n            mask_path = os.path.join(self.mask_dir, class_name, os.path.basename(img_path))  # Use the exact name\n            if not os.path.exists(mask_path):\n                raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n\n            mask = Image.open(mask_path).convert('L')  # Grayscale for masks\n            if self.transform_mask:\n                mask = self.transform_mask(mask)\n            if mask.shape[0] == 1:\n                mask = mask.repeat(3, 1, 1)\n\n            # Convert the image to 3 channels if it's not already\n            if img.shape[0] == 1:\n                img = img.repeat(3, 1, 1)\n            mask_rgb = torch.stack([mask.squeeze(0)] * 3, dim=0)  # Convert single-channel mask to 3-channel\n            img_with_mask = img * mask_rgb\n            return mask, label\n\n    # Define directories for train and test datasets\n    train_data_dir = os.path.join(data_dir, \"train\")\n    test_data_dir = os.path.join(data_dir, \"test\")\n    train_mask_dir = os.path.join(mask_dir, \"train\")\n    test_mask_dir = os.path.join(mask_dir, \"test\")\n\n    # Instantiate train and test datasets\n    train_dataset = CombinedDataset(train_data_dir, train_mask_dir, image_transform, mask_transform)\n    test_dataset = CombinedDataset(test_data_dir, test_mask_dir, test_transform, mask_transform)\n\n    # Split train dataset into train and validation sets\n    train_size = int(0.8 * len(train_dataset))\n    val_size = len(train_dataset) - train_size\n    train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n\n    # Create DataLoaders\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n    validation_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_loader, validation_loader, test_loader\n\n# Example usage\ndata_dir = \"/kaggle/working/pediatric_pneumonia/Pediatric Chest X-ray Pneumonia\"  # Path to images\nmask_dir = \"/kaggle/working/Kermany_masks\"   # Path to masks\nbatch_size = 8\n\ntrain_loader, validation_loader, test_loader = get_combined_data_loaders(data_dir, mask_dir, batch_size)\n\nprint(\"Success\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n# Train and validate the model\nbest_val_acc = 0.0\nbest_val_f1 = 0.0  # Initialize best_val_f1 here\n# Helper function to count parameters\ndef count_parameters(model, trainable_only=False):\n    if trainable_only:\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    else:\n        return sum(p.numel() for p in model.parameters())\n\n# Training function\ndef train(model, train_loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []  # Store all predictions\n    all_targets = []  # Store all targets\n\n    for batch_idx, (inputs, targets) in enumerate(train_loader):  # Modified to unpack three values\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n\n        loss = criterion(outputs, targets)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        # Accumulate predictions and targets\n        all_preds.extend(predicted.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())\n\n    train_loss = running_loss / len(train_loader)\n    train_acc = 100. * correct / total\n    return train_loss, train_acc, all_preds, all_targets  # Return 4 values\n\n\n# Validation function\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []  # Store all predictions\n    all_targets = []  # Store all targets\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:  # Modified to unpack three values\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            outputs = model(inputs)\n\n            loss = criterion(outputs, targets)\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            # Accumulate predictions and targets\n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    val_loss = running_loss / len(val_loader)\n    val_acc = 100. * correct / total\n    return val_loss, val_acc, all_preds, all_targets  # Return 4 values\n\n# # Define dataset location and parameters\n# data_dir = \"./Kermany_masks\"  # Ensure this is where you download/extract your dataset\n# batch_size = 32\ndata_dir = \"/kaggle/working/pediatric_pneumonia/Pediatric Chest X-ray Pneumonia\"  # Path to images\nmask_dir = \"/kaggle/working/Kermany_masks\"   # Path to masks\nbatch_size = 32\n\ntrain_loader,val_loader, test_loader = get_combined_data_loaders(data_dir, mask_dir, batch_size)\n\nlearning_rate = 5e-3\nnum_epochs = 2\nlog_dir = \"./logs\"\n\n# # Create DataLoader\n# train_loader, test_loader = get_data_loader(data_dir, batch_size)\n\n# Instantiate the MSR model\nmodel = MSR(layers=50, num_classes=2)  # Adjust for 3 classes: COVID19, NORMAL, PNEUMONIA\n\n# Count parameters\ntotal_params = count_parameters(model)\ntrainable_params = count_parameters(model, trainable_only=True)\nprint(f\"\\nBackbone # param.: {total_params}\")\nprint(f\"Learnable # param.: {trainable_params}\\n\")\n\n# Setup device and move model to GPU if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam([\n    {'params': model.get_backbone_params(), 'lr': learning_rate},\n    {'params': model.get_fc_params(), 'lr': learning_rate * 10}\n])\n\n# Train and validate the model\nbest_val_acc = 0.0\nos.makedirs(log_dir, exist_ok=True)\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef calculate_metrics(y_true, y_pred, average='weighted'):\n    precision = precision_score(y_true, y_pred, average=average)\n    recall = recall_score(y_true, y_pred, average=average)\n    f1 = f1_score(y_true, y_pred, average=average)\n    return precision, recall, f1\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    start_time = time.time()\n\n    # Training Phase\n    train_loss, train_acc, train_preds, train_targets = train(model, train_loader, optimizer, criterion, device)\n    train_precision, train_recall, train_f1 = calculate_metrics(train_targets, train_preds)\n\n    # Validation Phase\n    val_loss, val_acc, val_preds, val_targets = validate(model, val_loader, criterion, device)\n    val_precision, val_recall, val_f1 = calculate_metrics(val_targets, val_preds)\n\n    end_time = time.time()\n    epoch_time = end_time - start_time\n\n    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n    print(f\"Train Precision: {train_precision:.2f}, Train Recall: {train_recall:.2f}, Train F1-Score: {train_f1:.2f}\")\n\n    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n    print(f\"Validation Precision: {val_precision:.2f}, Validation Recall: {val_recall:.2f}, Validation F1-Score: {val_f1:.2f}\")\n\n    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds\\n\")\n\n    # Save the best model based on F1-Score\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), os.path.join(log_dir, \"best_model.pth\"))\n        print(f\"Best model saved with F1-Score: {best_val_f1:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T01:58:47.963045Z","iopub.execute_input":"2024-12-09T01:58:47.963378Z","iopub.status.idle":"2024-12-09T02:12:31.492228Z","shell.execute_reply.started":"2024-12-09T01:58:47.963350Z","shell.execute_reply":"2024-12-09T02:12:31.491174Z"}},"outputs":[{"name":"stdout","text":"\nBackbone # param.: 25431106\nLearnable # param.: 1976194\n\nEpoch 1/2\nTrain Loss: 6.5801, Train Accuracy: 74.36%\nTrain Precision: 0.74, Train Recall: 0.74, Train F1-Score: 0.74\nValidation Loss: 4.2706, Validation Accuracy: 79.66%\nValidation Precision: 0.83, Validation Recall: 0.80, Validation F1-Score: 0.74\nEpoch 1 completed in 396.79 seconds\n\nBest model saved with F1-Score: 0.74\nEpoch 2/2\nTrain Loss: 5.5963, Train Accuracy: 75.87%\nTrain Precision: 0.76, Train Recall: 0.76, Train F1-Score: 0.76\nValidation Loss: 4.1361, Validation Accuracy: 72.68%\nValidation Precision: 0.86, Validation Recall: 0.73, Validation F1-Score: 0.75\nEpoch 2 completed in 425.67 seconds\n\nBest model saved with F1-Score: 0.75\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# import torch\n# from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n# import torch.nn as nn\n\n# log_dir = \"./logs\"\n# os.makedirs(log_dir, exist_ok=True)\n# model = MSR(layers=50, num_classes=2)  # Adjust for 3 classes: COVID19, NORMAL, PNEUMONIA\n\n# def test_model(model, test_loader, criterion, device):\n#     model.eval()\n#     running_loss = 0.0\n#     correct = 0\n#     total = 0\n#     all_preds = []\n#     all_targets = []\n\n#     with torch.no_grad():\n#         for inputs, targets in test_loader:\n#             inputs, targets = inputs.to(device), targets.to(device)\n\n#             outputs = model(inputs)\n#             loss = criterion(outputs, targets)\n\n#             running_loss += loss.item()\n#             _, predicted = outputs.max(1)\n#             total += targets.size(0)\n#             correct += predicted.eq(targets).sum().item()\n\n#             # Accumulate predictions and targets\n#             all_preds.extend(predicted.cpu().numpy())\n#             all_targets.extend(targets.cpu().numpy())\n\n#     test_loss = running_loss / len(test_loader)\n#     test_acc = 100. * correct / total\n\n#     # Calculate precision, recall, F1-score\n#     # precision = precision_score(all_targets, all_preds, average='weighted')\n#     # recall = recall_score(all_targets, all_preds, average='weighted')\n#     # f1 = f1_score(all_targets, all_preds, average='weighted')\n\n#     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n\n#     return test_loss, test_acc, all_preds, all_targets\n\n# # precision, recall, f1, , test_recall, test_f1\n\n# # Load the best model\n# #best_model_path = os.path.join(log_dir, \"best_model.pth\")\n# best_model_path = \"/kaggle/input/class_best_base/pytorch/default/1/best_model.pth\"\n# model.load_state_dict(torch.load(best_model_path))\n# model.to(device)\n\ncriterion = nn.CrossEntropyLoss() \n# # Test the model\n# test_loss, test_acc, test_precision, all_preds, all_targets = test_model(model, test_loader, criterion, device)\n\n# # Print the results\n# print(f\"Test Loss: {test_loss:.4f}\")\n# print(f\"Test Accuracy: {test_acc:.2f}%\")\n# # print(f\"Test Precision: {test_precision:.2f}\")\n# # print(f\"Test Recall: {test_recall:.2f}\")\n# # print(f\"Test F1-Score: {test_f1:.2f}\")\n\ndef test(model, test_loader, criterion, device):\n    \"\"\"\n    Test the model on the test dataset.\n\n    Args:\n        model: Trained model to evaluate.\n        test_loader: DataLoader for the test dataset.\n        criterion: Loss function used for evaluation.\n        device: Device to perform the computations (CPU or GPU).\n\n    Returns:\n        test_loss: Average test loss.\n        test_acc: Test accuracy as a percentage.\n        all_preds: All predicted labels.\n        all_targets: All ground truth labels.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []  # Store all predictions\n    all_targets = []  # Store all targets\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:  # Get inputs and targets from test loader\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            outputs = model(inputs)  # Forward pass \n\n            loss = criterion(outputs, targets)  # Compute loss\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)  # Get the predicted class\n            total += targets.size(0)  # Total number of samples\n            correct += predicted.eq(targets).sum().item()  # Count correct predictions\n\n            # Accumulate predictions and targets\n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    test_loss = running_loss / len(test_loader)  # Compute average loss\n    test_acc = 100. * correct / total  # Compute accuracy\n\n    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n\n    return test_loss, test_acc, all_preds, all_targets  # Return test metrics and predictions\n\ntest_loss, test_acc, all_preds, all_targets = test(model, test_loader, criterion, device)\n\n# Print results\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}