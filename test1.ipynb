{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":193188,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":164763,"modelId":187090},{"sourceId":193193,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":164767,"modelId":187094}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download Dataset","metadata":{}},{"cell_type":"code","source":"!pip install kaggle timm --quiet\n\n!mkdir -p ~/.kaggle\n!echo '{\"username\":\"hafijulhoque987 \",\"key\":\"ba267fc402273b17f82059844f85fe32\"}' > ~/.kaggle/kaggle.json\n!chmod 600 ~/.kaggle/kaggle.json\n\n# Download the dataset\n!kaggle datasets download -d andrewmvd/pediatric-pneumonia-chest-xray\n\n# Extract the dataset\nimport zipfile\n\nwith zipfile.ZipFile('pediatric-pneumonia-chest-xray.zip', 'r') as zip_ref:\n    zip_ref.extractall('./pediatric_pneumonia')\n\nprint(\"Dataset downloaded and extracted successfully!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:26:02.729077Z","iopub.execute_input":"2024-12-12T05:26:02.729953Z","iopub.status.idle":"2024-12-12T05:26:36.248696Z","shell.execute_reply.started":"2024-12-12T05:26:02.729914Z","shell.execute_reply":"2024-12-12T05:26:36.247621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n# Set up device for training (GPU if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:26:54.482040Z","iopub.execute_input":"2024-12-12T05:26:54.482720Z","iopub.status.idle":"2024-12-12T05:26:57.553870Z","shell.execute_reply.started":"2024-12-12T05:26:54.482682Z","shell.execute_reply":"2024-12-12T05:26:57.552993Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transformer Classes","metadata":{}},{"cell_type":"code","source":"import copy\nimport math\nfrom typing import List, Optional\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\n\nclass PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n\n    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        if scale is not None and normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n        if scale is None:\n            scale = 2 * math.pi\n        self.scale = scale\n\n        # Multi-head attention module\n        # self.attention = nn.MultiheadAttention(embed_dim=2 * num_pos_feats, num_heads=num_heads)\n\n    def forward(self, x, mask=None):\n        if mask is None:\n            mask = torch.zeros((x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool)\n        not_mask = ~mask\n        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n        if self.normalize:\n            eps = 1e-6\n            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n\n        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n\n        pos_x = x_embed[:, :, :, None] / dim_t\n        pos_y = y_embed[:, :, :, None] / dim_t\n        pos_x = torch.stack(\n            (pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4\n        ).flatten(3)\n        pos_y = torch.stack(\n            (pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4\n        ).flatten(3)\n        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n        return pos\n\n        \n        # # Flatten for attention: [batch_size, 2*num_pos_feats, height, width] -> [height*width, batch_size, 2*num_pos_feats]\n        # pos_flat = pos.flatten(2).permute(2, 0, 1)\n\n        # # Apply multi-head attention\n        # pos_weighted, _ = self.attention(pos_flat, pos_flat, pos_flat)\n\n        # # Reshape back to [batch_size, 2*num_pos_feats, height, width]\n        # pos_weighted = pos_weighted.permute(1, 2, 0).view(pos.size(0), pos.size(1), pos.size(2), pos.size(3))\n\n        # return pos_weighted\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        d_model=512,\n        nhead=8,\n        num_encoder_layers=6,\n        num_decoder_layers=6,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"relu\",\n        normalize_before=False,\n        return_intermediate_dec=False,\n    ):\n        super().__init__()\n\n        encoder_layer = TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout, activation, normalize_before\n        )\n        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n\n        decoder_layer = TransformerDecoderLayer(\n            d_model, nhead, dim_feedforward, dropout, activation, normalize_before\n        )\n        decoder_norm = nn.LayerNorm(d_model)\n        self.decoder = TransformerDecoder(\n            decoder_layer,\n            num_decoder_layers,\n            decoder_norm,\n            return_intermediate=return_intermediate_dec,\n        )\n\n        self._reset_parameters()\n\n        self.d_model = d_model\n        self.nhead = nhead\n\n    def _reset_parameters(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, src, mask, query_embed, query_pos, value_pos):\n        # flatten NxCxHxW to HWxNxC\n        bs, c, l = query_embed.shape\n        query_embed = query_embed.permute(2, 0, 1)\n        query_pos = query_pos.unsqueeze(0).expand(bs,-1,-1).permute(2, 0, 1)\n        value=src.flatten(2).permute(2, 0, 1)\n        value_pos=value_pos.flatten(2).permute(2, 0, 1)\n        if mask is not None:\n            mask = mask.flatten(1)\n\n        #tgt = torch.zeros_like(query_embed)\n        memory = self.encoder(value, src_key_padding_mask=mask, pos=value_pos)\n        hs = self.decoder(\n            tgt=query_embed, memory=memory, memory_key_padding_mask=mask, pos=value_pos, query_pos=query_pos\n        )\n        return hs.transpose(1, 2).transpose(0, 2)\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, encoder_layer, num_layers, norm=None):\n        super().__init__()\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n\n    def forward(\n        self,\n        src,\n        mask: Optional[Tensor] = None,\n        src_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n    ):\n        output = src\n\n        for layer in self.layers:\n            output = layer(\n                output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, pos=pos\n            )\n\n        if self.norm is not None:\n            output = self.norm(output)\n\n        return output\n\n\nclass TransformerDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n        self.return_intermediate = return_intermediate\n\n    def forward(\n        self,\n        tgt,\n        memory,\n        tgt_mask: Optional[Tensor] = None,\n        memory_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        memory_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n        query_pos: Optional[Tensor] = None,\n    ):\n        output = tgt\n\n        intermediate = []\n\n        for layer in self.layers:\n            output = layer(\n                tgt=output,\n                memory=memory,\n                tgt_mask=tgt_mask,\n                memory_mask=memory_mask,\n                tgt_key_padding_mask=tgt_key_padding_mask,\n                memory_key_padding_mask=memory_key_padding_mask,\n                pos=pos,\n                query_pos=query_pos,\n            )\n            if self.return_intermediate:\n                intermediate.append(self.norm(output))\n\n        if self.norm is not None:\n            output = self.norm(output)\n            if self.return_intermediate:\n                intermediate.pop()\n                intermediate.append(output)\n\n        if self.return_intermediate:\n            return torch.stack(intermediate)\n\n        return output\n\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"relu\",\n        normalize_before=False,\n    ):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n\n    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n        return tensor if pos is None else tensor + pos\n\n    def forward_post(\n        self,\n        src,\n        src_mask: Optional[Tensor] = None,\n        src_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n    ):\n        q = k = self.with_pos_embed(src, pos)\n        src2 = self.self_attn(\n            q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n        )[0]\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src\n\n    def forward_pre(\n        self,\n        src,\n        src_mask: Optional[Tensor] = None,\n        src_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n    ):\n        src2 = self.norm1(src)\n        q = k = self.with_pos_embed(src2, pos)\n        src2 = self.self_attn(\n            q, k, value=src2, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n        )[0]\n        src = src + self.dropout1(src2)\n        src2 = self.norm2(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n        src = src + self.dropout2(src2)\n        return src\n\n    def forward(\n        self,\n        src,\n        src_mask: Optional[Tensor] = None,\n        src_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n    ):\n        if self.normalize_before:\n            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n\n\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"relu\",\n        normalize_before=False,\n    ):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n\n    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n        return tensor if pos is None else tensor + pos\n\n    def forward_post(\n        self,\n        tgt,\n        memory,\n        tgt_mask: Optional[Tensor] = None,\n        memory_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        memory_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n        query_pos: Optional[Tensor] = None,\n    ):\n        q = k = self.with_pos_embed(tgt, query_pos)\n        tgt2 = self.self_attn(\n            q, k, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask\n        )[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(\n            query=self.with_pos_embed(tgt, query_pos),\n            key=self.with_pos_embed(memory, pos),\n            value=memory,\n            attn_mask=memory_mask,\n            key_padding_mask=memory_key_padding_mask,\n        )[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt\n\n    def forward_pre(\n        self,\n        tgt,\n        memory,\n        tgt_mask: Optional[Tensor] = None,\n        memory_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        memory_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n        query_pos: Optional[Tensor] = None,\n    ):\n        tgt2 = self.norm1(tgt)\n        q = k = self.with_pos_embed(tgt2, query_pos)\n        tgt2 = self.self_attn(\n            q, k, value=tgt2, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask\n        )[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt2 = self.norm2(tgt)\n        tgt2 = self.multihead_attn(\n            query=self.with_pos_embed(tgt2, query_pos),\n            key=self.with_pos_embed(memory, pos),\n            value=memory,\n            attn_mask=memory_mask,\n            key_padding_mask=memory_key_padding_mask,\n        )[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt2 = self.norm3(tgt)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n        tgt = tgt + self.dropout3(tgt2)\n        return tgt\n\n    def forward(\n        self,\n        tgt,\n        memory,\n        tgt_mask: Optional[Tensor] = None,\n        memory_mask: Optional[Tensor] = None,\n        tgt_key_padding_mask: Optional[Tensor] = None,\n        memory_key_padding_mask: Optional[Tensor] = None,\n        pos: Optional[Tensor] = None,\n        query_pos: Optional[Tensor] = None,\n    ):\n        if self.normalize_before:\n            return self.forward_pre(\n                tgt,\n                memory,\n                tgt_mask,\n                memory_mask,\n                tgt_key_padding_mask,\n                memory_key_padding_mask,\n                pos,\n                query_pos,\n            )\n        return self.forward_post(\n            tgt,\n            memory,\n            tgt_mask,\n            memory_mask,\n            tgt_key_padding_mask,\n            memory_key_padding_mask,\n            pos,\n            query_pos,\n        )\n\n\ndef _get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n\n\ndef _get_activation_fn(activation):\n    \"\"\"Return an activation function given a string\"\"\"\n    if activation == \"relu\":\n        return F.relu\n    if activation == \"gelu\":\n        return F.gelu\n    if activation == \"glu\":\n        return F.glu\n    raise RuntimeError(f\"activation should be relu/gelu, not {activation}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:27:23.600340Z","iopub.execute_input":"2024-12-12T05:27:23.600677Z","iopub.status.idle":"2024-12-12T05:27:23.638066Z","shell.execute_reply.started":"2024-12-12T05:27:23.600645Z","shell.execute_reply":"2024-12-12T05:27:23.637088Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# UNET Classes","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels, out_channels, feature_dims):\n        super(UNet, self).__init__()\n\n        self.encoder1 = DoubleConv(in_channels, feature_dims)\n        self.encoder2 = DoubleConv(feature_dims, feature_dims * 2)\n        self.encoder3 = DoubleConv(feature_dims * 2, feature_dims * 4)\n        self.encoder4 = DoubleConv(feature_dims * 4, feature_dims * 8)\n\n        self.embed = nn.Embedding(feature_dims * 16,1)\n\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.pe_layer = PositionEmbeddingSine(feature_dims * 16 // 2, normalize=True)\n        self.transformer = Transformer(\n            d_model=feature_dims * 16,\n            dropout=0.1,\n            nhead=4,\n            dim_feedforward=feature_dims * 4,\n            num_encoder_layers=0,\n            num_decoder_layers=1,\n            normalize_before=False,\n            return_intermediate_dec=False,\n        )\n\n        self.bottleneck_conv = DoubleConv(feature_dims * 8, feature_dims * 16)\n\n        self.upconv4 = nn.ConvTranspose2d(feature_dims * 16, feature_dims * 8, kernel_size=2, stride=2)\n        self.decoder4 = DoubleConv(feature_dims * 16, feature_dims * 8)\n        self.upconv3 = nn.ConvTranspose2d(feature_dims * 8, feature_dims * 4, kernel_size=2, stride=2)\n        self.decoder3 = DoubleConv(feature_dims * 8, feature_dims * 4)\n        self.upconv2 = nn.ConvTranspose2d(feature_dims * 4, feature_dims * 2, kernel_size=2, stride=2)\n        self.decoder2 = DoubleConv(feature_dims * 4, feature_dims * 2)\n        self.upconv1 = nn.ConvTranspose2d(feature_dims * 2, feature_dims, kernel_size=2, stride=2)\n        self.decoder1 = DoubleConv(feature_dims * 2, feature_dims)\n\n        self.final_conv = nn.Conv2d(feature_dims, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        # Encoder\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(self.pool(enc1))\n        enc3 = self.encoder3(self.pool(enc2))\n        enc4 = self.encoder4(self.pool(enc3))\n\n        # Bottleneck\n        bottleneck = self.pool(enc4)\n        bottleneck = self.bottleneck_conv(bottleneck)\n\n        b, c, h, w = bottleneck.shape\n        bottleneck_avg = bottleneck.mean(dim=(2, 3), keepdim=True).squeeze(-1)\n        bottleneck_pe = self.pe_layer(bottleneck)\n\n        transformer_output = self.transformer(bottleneck, None, bottleneck_avg, self.embed.weight, bottleneck_pe) # (bsz,1024,1)\n\n        transformer_output_expanded = transformer_output.unsqueeze(-1) # (bsz,1024,1,1)\n        bottleneck = bottleneck * transformer_output_expanded\n\n        # Decoder\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n        return torch.sigmoid(self.final_conv(dec1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:28:29.343189Z","iopub.execute_input":"2024-12-12T05:28:29.343564Z","iopub.status.idle":"2024-12-12T05:28:29.356583Z","shell.execute_reply.started":"2024-12-12T05:28:29.343527Z","shell.execute_reply":"2024-12-12T05:28:29.355717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Backbone","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.models import resnet50, resnet101\nfrom torchvision.models._utils import IntermediateLayerGetter\n\nclass FrozenBatchNorm2d(torch.nn.Module):\n    def __init__(self, n):\n        super(FrozenBatchNorm2d, self).__init__()\n        self.register_buffer(\"weight\", torch.ones(n))\n        self.register_buffer(\"bias\", torch.zeros(n))\n        self.register_buffer(\"running_mean\", torch.zeros(n))\n        self.register_buffer(\"running_var\", torch.ones(n))\n\n    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs):\n        num_batches_tracked_key = prefix + 'num_batches_tracked'\n        if num_batches_tracked_key in state_dict:\n            del state_dict[num_batches_tracked_key]\n\n        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict,\n            missing_keys, unexpected_keys, error_msgs)\n\n    def forward(self, x):\n        w = self.weight.reshape(1, -1, 1, 1)\n        b = self.bias.reshape(1, -1, 1, 1)\n        rv = self.running_var.reshape(1, -1, 1, 1)\n        rm = self.running_mean.reshape(1, -1, 1, 1)\n        eps = 1e-5\n        scale = w * (rv + eps).rsqrt()\n        bias = b - rm * scale\n        return x * scale + bias\n\n\nclass BackboneBase(nn.Module):\n    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n        super().__init__()\n        for name, parameter in backbone.named_parameters():\n            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n                parameter.requires_grad_(False)\n        if return_interm_layers:\n            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n        else:\n            return_layers = {'layer4': \"0\"}\n        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n        self.num_channels = num_channels\n\n    def forward(self, x):\n        x = self.body(x)\n        return x\n\nresnets_dict = {\n    'resnet50': resnet50,\n    'resnet101': resnet101,\n}\n\nclass Backbone(BackboneBase):\n    def __init__(self, name: str, train_backbone: bool, return_interm_layers: bool, dilation: list):\n        backbone = resnets_dict[name](\n            pretrained=True, replace_stride_with_dilation=dilation, norm_layer=FrozenBatchNorm2d\n        )\n        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n        super().__init__(backbone, train_backbone, num_channels, return_interm_layers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:28:32.660237Z","iopub.execute_input":"2024-12-12T05:28:32.660813Z","iopub.status.idle":"2024-12-12T05:28:33.792142Z","shell.execute_reply.started":"2024-12-12T05:28:32.660780Z","shell.execute_reply":"2024-12-12T05:28:33.791221Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MSR Classes","metadata":{}},{"cell_type":"markdown","source":"## New base ++","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom timm import create_model\nfrom torchvision.models import vit_b_16\n\n\nclass MSR(nn.Module):\n    def __init__(self, layers=50, num_classes=2, reduce_dim=256):\n        super(MSR, self).__init__()\n\n        self.backbone = Backbone(\n            'resnet{}'.format(layers),\n            train_backbone=False,\n            return_interm_layers=True,\n            dilation=[False, True, True]\n        )\n        \n        # Use Swin Transformer as the backbone\n        self.backbone2 = create_model(\n            'swin_base_patch4_window7_224', \n            pretrained=True, \n            features_only=False\n        )\n\n        # ViT Backbone\n        self.backbone3 = create_model(\n            'vit_base_patch16_224',  # Base ViT model\n            pretrained=True,         # Use pre-trained weights\n            img_size=512,            # Update input size to 512x512\n            num_classes=num_classes  # Number of output classes\n            \n        )\n        self.backbone3.head = nn.Identity()\n        # self.backbone3 = vit_b_16(pretrained=True)\n\n        self.embed_cat=nn.Embedding(reduce_dim,1)\n        self.embed_3=nn.Embedding(reduce_dim,1)\n        self.embed_2=nn.Embedding(reduce_dim//2,1)\n        self.embed_1=nn.Embedding(reduce_dim//2,1)\n\n        self.pe_layer_cat=PositionEmbeddingSine(reduce_dim//2, normalize=True)\n        self.pe_layer_3=PositionEmbeddingSine(reduce_dim//2, normalize=True)\n        self.pe_layer_2=PositionEmbeddingSine(reduce_dim//4, normalize=True)\n        self.pe_layer_1=PositionEmbeddingSine(reduce_dim//4, normalize=True)\n\n        self.transformer_cat = Transformer(\n            d_model=reduce_dim,\n            dropout=0.1,\n            nhead=4,\n            dim_feedforward=reduce_dim//4,\n            num_encoder_layers=0,\n            num_decoder_layers=1,\n            normalize_before=False,\n            return_intermediate_dec=False,\n        )\n\n        self.transformer_3 = Transformer(\n            d_model=reduce_dim,\n            dropout=0.1,\n            nhead=4,\n            dim_feedforward=reduce_dim//4,\n            num_encoder_layers=0,\n            num_decoder_layers=1,\n            normalize_before=False,\n            return_intermediate_dec=False,\n        )\n        self.transformer_2 = Transformer(\n            d_model=reduce_dim//2,\n            dropout=0.1,\n            nhead=4,\n            dim_feedforward=reduce_dim//4,\n            num_encoder_layers=0,\n            num_decoder_layers=1,\n            normalize_before=False,\n            return_intermediate_dec=False,\n        )\n        self.transformer_1 = Transformer(\n            d_model=reduce_dim//2,\n            dropout=0.1,\n            nhead=4,\n            dim_feedforward=reduce_dim//4,\n            num_encoder_layers=0,\n            num_decoder_layers=1,\n            normalize_before=False,\n            return_intermediate_dec=False,\n        )\n\n\n        # Dimensionality reduction\n        self.conv_red_1 = nn.Sequential(\n            nn.Conv2d(512, reduce_dim // 2, kernel_size=1, bias=False),\n            nn.BatchNorm2d(reduce_dim // 2),\n            nn.ReLU(inplace=True)\n        )\n        self.conv_red_2 = nn.Sequential(\n            nn.Conv2d(1024, reduce_dim // 2, kernel_size=1, bias=False),\n            nn.BatchNorm2d(reduce_dim // 2),\n            nn.ReLU(inplace=True)\n        )\n        self.conv_red_3 = nn.Sequential(\n            nn.Conv2d(2048, reduce_dim, kernel_size=1, bias=False),\n            nn.BatchNorm2d(reduce_dim),\n            nn.ReLU(inplace=True)\n        )\n\n        # Pooling and classification layers\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(reduce_dim * 2, reduce_dim)\n        self.fc_2 = nn.Linear(reduce_dim, num_classes)\n        self.fc_3 = nn.Linear(reduce_dim // 2, num_classes)\n        # self.fc_4 = nn.Linear(1000, num_classes)\n        # self.fc_5 = nn.Linear(768, num_classes)\n\n        # Learnable dynamic weights for output fusion\n        self.dynamic_weights = nn.Parameter(torch.tensor([1.0, 1.0, 1.0], requires_grad=True))\n\n    def forward(self, x):\n        # Backbone feature extraction\n        # print(\"x shape\", x.shape)\n        \n        back_x = self.backbone(x)\n        # back_x2 = self.backbone2(x)\n        # back_x3 = self.backbone3(x)\n        \n        # print(\"back_x3: \", back_x3.shape)\n        \n        # return back_x\n\n        # print(\"backx 1 \", back_x['1'].shape)\n\n        # Process each stage of Swin features\n        red_back_1 = self.conv_red_1(back_x['1'])\n        avg_back_1 = self.avgpool(red_back_1)\n        red_back_2 = self.conv_red_2(back_x['2'])\n        avg_back_2 = self.avgpool(red_back_2)\n        red_back_3 = self.conv_red_3(back_x['3'])\n        avg_back_3 = self.avgpool(red_back_3)\n\n        red_back_cat = torch.cat((red_back_1, red_back_2), dim=1)\n        avg_back_cat = self.avgpool(red_back_cat)\n\n\n        # print(\"red_back_1 shape:\",red_back_1.shape)\n        # print(\"red_back_2 shape:\",red_back_2.shape)\n        # print(\"red_back_3 shape:\",red_back_3.shape)\n        # print(\"red back cat shape:\",red_back_cat.shape)\n\n\n\n        masking=None\n        query_pos = self.embed_1.weight\n        key_embed = red_back_1\n        query_embed = avg_back_1.squeeze(-1)\n        key_pos = self.pe_layer_1(red_back_1)\n\n        fg_embed_1=self.transformer_1(key_embed,masking,query_embed,query_pos,key_pos)\n        # print(\"shape1: \",fg_embed_1.shape)\n\n        query_pos = self.embed_2.weight\n        key_embed = red_back_2\n        query_embed = avg_back_2.squeeze(-1)\n        key_pos = self.pe_layer_2(red_back_1)\n\n        fg_embed_2=self.transformer_2(key_embed,masking,query_embed,query_pos,key_pos)\n        # print(\"shape2: \",fg_embed_2.shape)\n\n\n        query_pos = self.embed_3.weight\n        key_embed = red_back_3\n        query_embed = avg_back_3.squeeze(-1)\n        key_pos = self.pe_layer_3(red_back_3)\n        fg_embed_3=self.transformer_3(key_embed,masking,query_embed,query_pos,key_pos)\n        # print(\"shape3: \",fg_embed_3.shape)\n\n\n\n\n        query_pos = self.embed_cat.weight\n        key_embed = red_back_cat\n        query_embed = avg_back_cat.squeeze(-1)\n        key_pos = self.pe_layer_cat(red_back_cat)\n\n        fg_embed_cat=self.transformer_cat(key_embed,masking,query_embed,query_pos,key_pos)\n        # print(\"shape cat: \",fg_embed_cat.shape)\n\n\n\n\n        out = torch.cat((fg_embed_cat, fg_embed_3), dim=1)\n        # print(\"out: \",out.shape)\n\n\n        out_1 = torch.flatten(out, 1)  # Flatten the feature maps\n        # print(\"out1 flat: \",out_1.shape)\n\n        out_1 = self.fc(out_1)  # Fully connected layer\n        out_1 = self.fc_2(out_1)  # Fully connected layer\n        # print(\"out1 fc: \", out_1.shape)\n        out_2=torch.flatten(fg_embed_1,1)\n        # print(\"out2 : \",out_2.shape)\n        out_2=self.fc_3(out_2)\n\n        out_3=torch.flatten(fg_embed_2,1)\n        out_3=self.fc_3(out_3)\n        # print(\"out1: \",out_1.shape)\n        # print(\"out2: \",out_2.shape)\n        # print(\"out3: \",out_3.shape)\n        # print(\"output shape: \",back_x2.shape)\n        # out_4 = self.fc_4(back_x2)\n        # print(\"out4: \",out_4.shape)\n        # out_5 = self.fc_5(back_x3)\n        # print(\"out5: \",out_5.shape)\n        dynamic_weights = torch.softmax(self.dynamic_weights, dim=0)\n\n        # Weighted average of outputs\n        final_out = (\n            dynamic_weights[0] * out_1 +\n            dynamic_weights[1] * out_2 +\n            dynamic_weights[2] * out_3\n            # dynamic_weights[3] * out_5 \n            # dynamic_weights[4] * out_5\n        )\n\n        return final_out\n\n    def get_backbone_params(self):\n        return self.backbone.parameters()\n\n    def get_fc_params(self):\n        return self.fc.parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:53:31.744152Z","iopub.execute_input":"2024-12-12T08:53:31.744539Z","iopub.status.idle":"2024-12-12T08:53:37.101384Z","shell.execute_reply.started":"2024-12-12T08:53:31.744499Z","shell.execute_reply":"2024-12-12T08:53:37.100742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Old Base ++","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import vit_b_16\nfrom timm import create_model \n\n\nclass MSR1(nn.Module):\n    def __init__(self, layers, num_classes=2, reduce_dim=256):\n        super(MSR1, self).__init__()\n        self.backbone = Backbone(\n            'resnet{}'.format(layers),\n            train_backbone=False,\n            return_interm_layers=True,\n            dilation=[False, True, True]\n        )\n\n        # ViT Backbone\n        # self.backbone3 = vit_b_16(pretrained=True)\n        # self.backbone3.head = nn.Identity()  # Remove classification head\n        self.backbone3 = create_model(\n            'vit_base_patch16_224',  # Base ViT model\n            pretrained=True,         # Use pre-trained weights\n            img_size=512,            # Update input size to 512x512\n            num_classes=num_classes  # Number of output classes\n            \n        )\n        self.backbone3.head = nn.Identity()\n        \n        self.embed_cat=nn.Embedding(reduce_dim,1)\n        self.embed_3=nn.Embedding(reduce_dim,1)\n        self.pe_layer_cat=PositionEmbeddingSine(reduce_dim//2, normalize=True)\n        self.pe_layer_3=PositionEmbeddingSine(reduce_dim//2, normalize=True)\n\n        self.transformer_cat = Transformer(\n            d_model=reduce_dim,\n            dropout=0.1,\n            nhead=4,\n            dim_feedforward=reduce_dim//4,\n            num_encoder_layers=0,\n            num_decoder_layers=1,\n            normalize_before=False,\n            return_intermediate_dec=False,\n        )\n\n        self.transformer_3 = Transformer(\n            d_model=reduce_dim,\n            dropout=0.1,\n            nhead=4,\n            dim_feedforward=reduce_dim//4,\n            num_encoder_layers=0,\n            num_decoder_layers=1,\n            normalize_before=False,\n            return_intermediate_dec=False,\n        )\n        self.conv_red_1 = nn.Sequential(\n            nn.Conv2d(512, reduce_dim//2, kernel_size=1, padding=0, bias=False)\n        )\n        self.conv_red_2 = nn.Sequential(\n            nn.Conv2d(1024, reduce_dim//2, kernel_size=1, padding=0, bias=False)\n        )\n        self.conv_red_3 = nn.Sequential(\n            nn.Conv2d(2048, reduce_dim, kernel_size=1, padding=0, bias=False)\n        )\n\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(reduce_dim*2, reduce_dim)  # Assuming last feature map size\n        self.fc_2 = nn.Linear(reduce_dim, num_classes)  # Assuming last feature map size\n        self.fc_3 = nn.Linear(768, num_classes)\n        self.num_classes = num_classes\n\n        # Learnable dynamic weights for output fusion\n        self.dynamic_weights = nn.Parameter(torch.tensor([1.0, 1.0], requires_grad=True))\n\n    def forward(self, x):\n        # Backbone feature extraction\n        back_x = self.backbone(x)\n        # back_x2 = self.backbone3(x)\n        red_back_1 = self.conv_red_1(back_x['1'])\n        red_back_2 = self.conv_red_2(back_x['2'])\n\n        red_back_cat = torch.cat((red_back_1, red_back_2), dim=1)\n        avg_back_cat = self.avgpool(red_back_cat)\n\n        red_back_3 = self.conv_red_3(back_x['3'])\n        avg_back_3 = self.avgpool(red_back_3)\n\n        masking=None\n        query_pos = self.embed_cat.weight\n\n        key_embed = red_back_cat\n        query_embed = avg_back_cat.squeeze(-1)\n        key_pos = self.pe_layer_cat(red_back_cat)\n\n        fg_embed_cat=self.transformer_cat(key_embed,masking,query_embed,query_pos,key_pos)\n\n        query_pos = self.embed_3.weight\n        key_embed = red_back_3\n        query_embed = avg_back_3.squeeze(-1)\n        key_pos = self.pe_layer_3(red_back_3)\n        fg_embed_3=self.transformer_3(key_embed,masking,query_embed,query_pos,key_pos)\n\n\n        #out_back_cat = (torch.einsum(\"bchw,bcl->blhw\",red_back_cat,fg_embed_cat)).permute(0, 2, 3, 1)\n        #out_back_3 = (torch.einsum(\"bchw,bcl->blhw\",red_back_3,fg_embed_3)).permute(0, 2, 3, 1)\n\n\n\n        # Concatenate along the last dimension\n        out = torch.cat((fg_embed_cat, fg_embed_3), dim=1)\n\n        out_1 = torch.flatten(out, 1)  # Flatten the feature maps\n\n        out_1 = self.fc(out_1)  # Fully connected layer\n        out_1 = self.fc_2(out_1)  # Fully connected layer\n        out_2 = self.fc_3(back_x2)\n\n        dynamic_weights = torch.softmax(self.dynamic_weights, dim=0)\n\n        # Weighted average of outputs\n        final_out = (\n            dynamic_weights[0] * out_1 +\n            dynamic_weights[1] * out_2\n        )\n\n        return final_out\n\n    def get_backbone_params(self):\n        return self.backbone.parameters()\n\n    def get_fc_params(self):\n        return self.fc.parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:27:17.724589Z","iopub.execute_input":"2024-12-12T07:27:17.724953Z","iopub.status.idle":"2024-12-12T07:27:17.740688Z","shell.execute_reply.started":"2024-12-12T07:27:17.724922Z","shell.execute_reply":"2024-12-12T07:27:17.739833Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"img_res = 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:29:12.494628Z","iopub.execute_input":"2024-12-12T05:29:12.495540Z","iopub.status.idle":"2024-12-12T05:29:12.499216Z","shell.execute_reply.started":"2024-12-12T05:29:12.495501Z","shell.execute_reply":"2024-12-12T05:29:12.498345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\n\ndef get_data_loader(data_dir, batch_size):\n    # Define transformations\n    train_transform = transforms.Compose([\n        transforms.Resize((img_res, img_res)),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor()\n    ])\n\n    test_transform = transforms.Compose([\n        transforms.Resize((512, 512)),\n        transforms.ToTensor()\n    ])\n\n    # Load datasets\n    train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"train\"), transform=train_transform)\n    test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"test\"), transform=test_transform)\n\n\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n\n\n    return train_loader, test_loader\n\ndef get_test_loader(data_dir, batch_size):\n    transform = transforms.Compose([\n        transforms.Resize((img_res, img_res)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # Load test dataset\n    test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"test\"), transform=transform)\n\n\n    # Create test loader\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n\n\n    return test_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:29:40.883995Z","iopub.execute_input":"2024-12-12T05:29:40.884503Z","iopub.status.idle":"2024-12-12T05:29:40.892384Z","shell.execute_reply.started":"2024-12-12T05:29:40.884468Z","shell.execute_reply":"2024-12-12T05:29:40.891332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:29:45.595676Z","iopub.execute_input":"2024-12-12T05:29:45.596012Z","iopub.status.idle":"2024-12-12T05:29:59.053814Z","shell.execute_reply.started":"2024-12-12T05:29:45.595981Z","shell.execute_reply":"2024-12-12T05:29:59.052894Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate Masks","metadata":{}},{"cell_type":"code","source":"import pickle\nimport os\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport shutil  # For directory management\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n\n# Load the trained model from a .pkl file\nmodel_path = \"/kaggle/input/dhur/pytorch/default/1/best_model_base.pth\"\nmodel = UNet(in_channels=3, out_channels=3, feature_dims=16)  # Match your training parameters\nmodel.load_state_dict(torch.load(model_path))\nmodel.to(device)  # Move the model to the appropriate device (CPU or GPU)\n\nprint(\"Model loaded successfully from .pkl\")\n\nfor name, param in model.named_parameters():\n    print(f\"{name}: mean={param.mean().item()}, std={param.std().item()}\")\n\nfrom pathlib import Path\nPath('/kaggle/working/Kermany_masks/train').mkdir(parents=True, exist_ok=True)\nPath('/kaggle/working/Kermany_masks/test').mkdir(parents=True, exist_ok=True)        \n\n\n\ndef generate_masks(model, data_loader, device, output_dir, threshold=0.5):\n\n    # # Ensure the output directory is fresh each time\n    # # if os.path.exists(output_dir):\n    # #     shutil.rmtree(output_dir)\n    # if not os.path.exists(output_dir):\n    #     os.makedirs(output_dir)\n    os.makedirs(os.path.join(output_dir, \"NORMAL\"), exist_ok=True)\n    os.makedirs(os.path.join(output_dir, \"PNEUMONIA\"), exist_ok=True)\n\n    model.eval()\n    model.to(device)\n\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(data_loader):\n            # Convert inputs to grayscale if necessary\n          #  inputs = transforms.functional.rgb_to_grayscale(inputs, num_output_channels=1)\n\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            predictions = (outputs > threshold).float()\n\n\n            for j in range(inputs.size(0)):\n                label = targets[j].item()  # Get the integer label (0 or 1)\n\n                # Determine the save directory based on the label\n                if label == 0:\n                    class_dir = os.path.join(output_dir, \"NORMAL\")\n                elif label == 1:\n                    class_dir = os.path.join(output_dir, \"PNEUMONIA\")\n                else:\n                    raise ValueError(f\"Unexpected label value: {label}\")\n\n                # Get the original image filename\n                original_path, _ = data_loader.dataset.samples[batch_idx * data_loader.batch_size + j]\n                image_name = os.path.basename(original_path)  # Extract the original filename\n                #print(\"Image name\", image_name)\n                save_path = os.path.join(class_dir, image_name.replace(\".jpg\", \"_mask.png\"))\n\n                # Save the predicted mask with the original filename\n                pred_mask = predictions[j].cpu().numpy().squeeze()\n                pred_mask = (pred_mask > threshold).astype(np.uint8)\n                if pred_mask.ndim == 3:\n                    pred_mask = pred_mask[0]\n                 # Plot the mask for debugging\n                # plt.figure(figsize=(10, 5))\n                # plt.subplot(1, 2, 1)\n                # plt.title(\"Predicted Mask\")\n                # plt.imshow(pred_mask, cmap='gray')\n                # plt.axis(\"off\")\n\n                # plt.subplot(1, 2, 2)\n                # plt.title(\"Original Image\")\n                # plt.imshow(inputs[j].cpu().permute(1, 2, 0).numpy())\n                # plt.axis(\"off\")\n\n                # plt.show()\n\n\n                pred_mask = (pred_mask * 255).astype(np.uint8)  # Scale to 0-255\n                Image.fromarray(pred_mask).save(save_path)\n\n\n\n# Example Usage\n# data_dir = \"./kermany/kermany\"  # Ensure this is where your dataset is located\ndata_dir = \"/kaggle/working/pediatric_pneumonia/Pediatric Chest X-ray Pneumonia\"\nbatch_size = 8\n#one for train one for test\noutput_dir = \"/kaggle/working/Kermany_masks/test\"\noutput_dir1 = \"/kaggle/working/Kermany_masks/train\"\n\n# Assuming `segmentation_model` is your pre-trained model\ntrain_loader, test_loader = get_data_loader(data_dir, batch_size)\ngenerate_masks(model, test_loader, device, output_dir)\nprint(\"Test.\")\ngenerate_masks(model, train_loader, device, output_dir1)\nprint(\"Train\")\n\nprint(\"Output generated.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:30:04.303200Z","iopub.execute_input":"2024-12-12T05:30:04.303592Z","iopub.status.idle":"2024-12-12T05:31:52.167148Z","shell.execute_reply.started":"2024-12-12T05:30:04.303557Z","shell.execute_reply":"2024-12-12T05:31:52.165990Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing ","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import random_split\n\ndef get_combined_data_loaders(data_dir, mask_dir, batch_size):\n    image_transform = transforms.Compose([\n        transforms.Resize((img_res, img_res)),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor()\n    ])\n\n    mask_transform = transforms.Compose([\n        transforms.Resize((img_res, img_res)),\n        transforms.ToTensor()\n    ])\n    test_transform = transforms.Compose([\n        transforms.Resize((img_res, img_res)),\n        transforms.ToTensor()\n    ])\n\n    class CombinedDataset(torch.utils.data.Dataset):\n        def __init__(self, data_dir, mask_dir, transform_img, transform_mask):\n            self.image_dataset = datasets.ImageFolder(root=data_dir, transform=None)\n            self.mask_dir = mask_dir\n            self.transform_img = transform_img\n            self.transform_mask = transform_mask\n\n            self.samples = self.image_dataset.samples\n\n        def __len__(self):\n            return len(self.image_dataset)\n\n        def __getitem__(self, index):\n            img_path, label = self.samples[index]\n            img = Image.open(img_path).convert('RGB')\n            if self.transform_img:\n                img = self.transform_img(img)\n\n            # Match mask by exact filename\n            class_name = os.path.basename(os.path.dirname(img_path))\n            mask_path = os.path.join(self.mask_dir, class_name, os.path.basename(img_path))  # Use the exact name\n            if not os.path.exists(mask_path):\n                raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n\n            mask = Image.open(mask_path).convert('L')  # Grayscale for masks\n            if self.transform_mask:\n                mask = self.transform_mask(mask)\n            if mask.shape[0] == 1:\n                mask = mask.repeat(3, 1, 1)\n\n            # Convert the image to 3 channels if it's not already\n            if img.shape[0] == 1:\n                img = img.repeat(3, 1, 1)\n            mask_rgb = torch.stack([mask.squeeze(0)] * 3, dim=0)  # Convert single-channel mask to 3-channel\n            img_with_mask = img * mask_rgb\n            return mask, label\n\n    # Define directories for train and test datasets\n    train_data_dir = os.path.join(data_dir, \"train\")\n    test_data_dir = os.path.join(data_dir, \"test\")\n    train_mask_dir = os.path.join(mask_dir, \"train\")\n    test_mask_dir = os.path.join(mask_dir, \"test\")\n\n    # Instantiate train and test datasets\n    train_dataset = CombinedDataset(train_data_dir, train_mask_dir, image_transform, mask_transform)\n    test_dataset = CombinedDataset(test_data_dir, test_mask_dir, test_transform, mask_transform)\n\n    # Split train dataset into train and validation sets\n    train_size = int(0.8 * len(train_dataset))\n    val_size = len(train_dataset) - train_size\n    train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n\n    # Create DataLoaders\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n    validation_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_loader, validation_loader, test_loader\n\n# Example usage\ndata_dir = \"/kaggle/working/pediatric_pneumonia/Pediatric Chest X-ray Pneumonia\"  # Path to images\nmask_dir = \"/kaggle/working/Kermany_masks\"   # Path to masks\nbatch_size = 8\n\ntrain_loader, validation_loader, test_loader = get_combined_data_loaders(data_dir, mask_dir, batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:32:20.037336Z","iopub.execute_input":"2024-12-12T05:32:20.038182Z","iopub.status.idle":"2024-12-12T05:32:20.071124Z","shell.execute_reply.started":"2024-12-12T05:32:20.038149Z","shell.execute_reply":"2024-12-12T05:32:20.070536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train ","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n# Train and validate the model\nbest_val_acc = 0.0\nbest_val_f1 = 0.0  # Initialize best_val_f1 here\n# Helper function to count parameters\ndef count_parameters(model, trainable_only=False):\n    if trainable_only:\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    else:\n        return sum(p.numel() for p in model.parameters())\n\n# Training function\ndef train(model, train_loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []  # Store all predictions\n    all_targets = []  # Store all targets\n\n    for batch_idx, (inputs, targets) in enumerate(train_loader):  # Modified to unpack three values\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n\n\n\n\n        \n        loss = criterion(outputs, targets)\n\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        # Accumulate predictions and targets\n        all_preds.extend(predicted.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())\n\n    train_loss = running_loss / len(train_loader)\n    train_acc = 100. * correct / total\n    return train_loss, train_acc, all_preds, all_targets  # Return 4 values\n\n\n# Validation function\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []  # Store all predictions\n    all_targets = []  # Store all targets\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:  # Modified to unpack three values\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            outputs = model(inputs)\n\n            loss = criterion(outputs, targets)\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            # Accumulate predictions and targets\n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    val_loss = running_loss / len(val_loader)\n    val_acc = 100. * correct / total\n    return val_loss, val_acc, all_preds, all_targets  # Return 4 values\n\n# # Define dataset location and parameters\n# data_dir = \"./Kermany_masks\"  # Ensure this is where you download/extract your dataset\n# batch_size = 32\ndata_dir = \"/kaggle/working/pediatric_pneumonia/Pediatric Chest X-ray Pneumonia\"  # Path to images\nmask_dir = \"/kaggle/working/Kermany_masks\"   # Path to masks\nbatch_size = 64\n\ntrain_loader,val_loader, test_loader = get_combined_data_loaders(data_dir, mask_dir, batch_size)\n\nlearning_rate = 5e-3\nnum_epochs = 5\nlog_dir = \"./logs\"\n\n# # Create DataLoader\n# train_loader, test_loader = get_data_loader(data_dir, batch_size)\n\n# Instantiate the MSR model\nmodel = MSR1(layers=50, num_classes=2)  # Adjust for 3 classes: COVID19, NORMAL, PNEUMONIA\n# model = MSR(num_classes=2)  # Adjust for 3 classes: COVID19, NORMAL, PNEUMONIA\n\n# Count parameters\ntotal_params = count_parameters(model)\ntrainable_params = count_parameters(model, trainable_only=True)\nprint(f\"\\nBackbone # param.: {total_params}\")\nprint(f\"Learnable # param.: {trainable_params}\\n\")\n\n# Setup device and move model to GPU if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam([\n    {'params': model.get_backbone_params(), 'lr': learning_rate},\n    {'params': model.get_fc_params(), 'lr': learning_rate * 10},\n    {'params': model.dynamic_weights, 'lr': 0.01},\n])\n\n# Train and validate the model\nbest_val_acc = 0.0\nos.makedirs(log_dir, exist_ok=True)\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef calculate_metrics(y_true, y_pred, average='weighted'):\n    precision = precision_score(y_true, y_pred, average=average)\n    recall = recall_score(y_true, y_pred, average=average)\n    f1 = f1_score(y_true, y_pred, average=average)\n    return precision, recall, f1\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    start_time = time.time()\n\n    # Training Phase\n    train_loss, train_acc, train_preds, train_targets = train(model, train_loader, optimizer, criterion, device)\n    train_precision, train_recall, train_f1 = calculate_metrics(train_targets, train_preds)\n\n    # Validation Phase\n    val_loss, val_acc, val_preds, val_targets = validate(model, val_loader, criterion, device)\n    val_precision, val_recall, val_f1 = calculate_metrics(val_targets, val_preds)\n\n    end_time = time.time()\n    epoch_time = end_time - start_time\n\n    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n    print(f\"Train Precision: {train_precision:.2f}, Train Recall: {train_recall:.2f}, Train F1-Score: {train_f1:.2f}\")\n\n    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n    print(f\"Validation Precision: {val_precision:.2f}, Validation Recall: {val_recall:.2f}, Validation F1-Score: {val_f1:.2f}\")\n\n    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds\\n\")\n\n    # Save the best model based on F1-Score\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), os.path.join(log_dir, \"best_model.pth\"))\n        print(f\"Best model saved with F1-Score: {best_val_f1:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:54:04.638443Z","iopub.execute_input":"2024-12-12T08:54:04.638785Z","iopub.status.idle":"2024-12-12T08:54:05.082234Z","shell.execute_reply.started":"2024-12-12T08:54:04.638755Z","shell.execute_reply":"2024-12-12T08:54:05.081123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(model, test_loader, criterion, device):\n    \"\"\"\n    Test the model on the test dataset.\n\n    Args:\n        model: Trained model to evaluate.\n        test_loader: DataLoader for the test dataset.\n        criterion: Loss function used for evaluation.\n        device: Device to perform the computations (CPU or GPU).\n\n    Returns:\n        test_loss: Average test loss.\n        test_acc: Test accuracy as a percentage.\n        all_preds: All predicted labels.\n        all_targets: All ground truth labels.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []  # Store all predictions\n    all_targets = []  # Store all targets\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:  # Get inputs and targets from test loader\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            outputs = model(inputs)  # Forward pass\n\n            loss = criterion(outputs, targets)  # Compute loss\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)  # Get the predicted class\n            total += targets.size(0)  # Total number of samples\n            correct += predicted.eq(targets).sum().item()  # Count correct predictions\n\n            # Accumulate predictions and targets\n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    test_loss = running_loss / len(test_loader)  # Compute average loss\n    test_acc = 100. * correct / total  # Compute accuracy\n\n    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n\n    return test_loss, test_acc, all_preds, all_targets  # Return test metrics and predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:09:26.142111Z","iopub.execute_input":"2024-12-12T07:09:26.142555Z","iopub.status.idle":"2024-12-12T07:09:26.150410Z","shell.execute_reply.started":"2024-12-12T07:09:26.142518Z","shell.execute_reply":"2024-12-12T07:09:26.149534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()\n\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:34:51.400189Z","iopub.execute_input":"2024-12-12T06:34:51.400580Z","iopub.status.idle":"2024-12-12T06:34:51.930941Z","shell.execute_reply.started":"2024-12-12T06:34:51.400546Z","shell.execute_reply":"2024-12-12T06:34:51.930012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming you have already loaded the model and test_loader\n# model_path = \"/kaggle/input/ugh/pytorch/default/1/best_model.pth\"\n# model = MSR(layers=50, num_classes=2)  # Adjust for 3 classes: COVID19, NORMAL, PNEUMONIA\nmodel_path = os.path.join(log_dir, \"best_model.pth\")\nmodel.load_state_dict(torch.load(model_path))\nmodel.to(device)  # Move the model to the appropriate device (CPU or GPU)\n#criterion = nn.CrossEntropyLoss()\n\ntest_loss, test_acc, all_preds, all_targets = test(model, test_loader, criterion, device)\n\n# Print results\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.2f}%\")\n\n# If needed, you can use all_preds and all_targets for further analysis\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:34:51.444499Z","iopub.execute_input":"2024-12-12T07:34:51.445447Z","iopub.status.idle":"2024-12-12T07:35:37.955768Z","shell.execute_reply.started":"2024-12-12T07:34:51.445408Z","shell.execute_reply":"2024-12-12T07:35:37.954642Z"}},"outputs":[],"execution_count":null}]}